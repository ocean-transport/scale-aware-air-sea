{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbbcae8-e859-40ab-9cc3-3613f3ab5ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # distributed does not like these long tasks (>40s) we produce. \n",
    "# # Lets see if we can tune that (!YES!):\n",
    "# import dask\n",
    "\n",
    "# # logging_config = {\n",
    "# #     \"version\": 1,\n",
    "# #     \"handlers\": {\n",
    "# #         \"file\": {\n",
    "# #             \"class\": \"logging.handlers.RotatingFileHandler\",\n",
    "# #             \"filename\": \"output_test.log\",\n",
    "# #             \"level\": \"DEBUG\",\n",
    "# #         },\n",
    "# #         \"console\": {\n",
    "# #             \"class\": \"logging.StreamHandler\",\n",
    "# #             \"level\": \"DEBUG\",\n",
    "# #         }\n",
    "# #     },\n",
    "# #     \"loggers\": {\n",
    "# #         \"distributed.worker\": {\n",
    "# #             \"level\": \"DEBUG\",\n",
    "# #             \"handlers\": [\"file\", \"console\"],\n",
    "# #         },\n",
    "# #         \"distributed.scheduler\": {\n",
    "# #             \"level\": \"DEBUG\",\n",
    "# #             \"handlers\": [\"file\", \"console\"],\n",
    "# #         }\n",
    "# #     }\n",
    "# # }\n",
    "# # dask.config.config['logging'] = logging_config\n",
    "\n",
    "# dask.config.set(\n",
    "#     {\n",
    "#         \"distributed.comm.timeouts.tcp\": \"360s\",\n",
    "#         \"distributed.comm.timeouts.connect\": \"360s\",\n",
    "#         # Account for even longer duration of the taper filter?\n",
    "#         # \"distributed.scheduler.allowed-failures\" : 2, # not sure if this will help\n",
    "#         # \"distributed.comm.timeouts.tcp\": \"6000s\",\n",
    "#         # \"distributed.comm.timeouts.connect\": \"6000s\",\n",
    "#         # \"distributed.deploy.lost-worker-timeout\": \"6000s\",\n",
    "#         # \"distributed.scheduler.idle-timeout\": \"6000s\"\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # print(dask.config.get('logging'))\n",
    "\n",
    "# # # try a local cluster for testing\n",
    "# # from distributed import Client, LocalCluster\n",
    "# # cluster = LocalCluster(n_workers=4, threads_per_worker=4)\n",
    "# # client = Client(cluster)\n",
    "# # client\n",
    "\n",
    "# from dask_gateway import Gateway\n",
    "\n",
    "# gateway = Gateway()\n",
    "\n",
    "# # close existing clusters\n",
    "# open_clusters = gateway.list_clusters()\n",
    "# print(list(open_clusters))\n",
    "# if len(open_clusters)>0:\n",
    "#     for c in open_clusters:\n",
    "#         cluster = gateway.connect(c.name)\n",
    "#         cluster.shutdown()  \n",
    "\n",
    "# options = gateway.cluster_options()\n",
    "# options.worker_memory = 50 # Could proabably reduce this to something like 40, but I think that doesnt actually free up space...\n",
    "# options.worker_cores = 10 # could probably handle 12 here...\n",
    "\n",
    "# options.environment = dict(\n",
    "#     DASK_DISTRIBUTED__SCHEDULER__WORKER_SATURATION=\"1.0\"\n",
    "# )\n",
    "\n",
    "# # Create a cluster with those options\n",
    "# cluster = gateway.new_cluster(options)\n",
    "# client = cluster.get_client()\n",
    "# # cluster.adapt(10,200)\n",
    "# cluster.scale(4) # for testing\n",
    "# client"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
